{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "발표자료최종본.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cSlUeicQCKyz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anydevil0812/bootcamp_project/blob/main/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C%EC%B5%9C%EC%A2%85%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 개요\n",
        "- 프로젝트 주제: RNN을 활용한 장르별 노래 가사 생성 시스템\n",
        "- 데이터 출처: **멜론 사이트에서 장르별 50곡 가사 크롤링**\n",
        "- 한국어 형태소 분석기 konlpy의 Okt 사용하여 토큰화\n",
        "- LSTM 모델 적용"
      ],
      "metadata": {
        "id": "n1X-4HDScURM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 멜론 장르별 TOP 50 크롤링 하기\n",
        "  - 원하는 장르를 선택하고 TOP 50을 크롤링 합니다.\n"
      ],
      "metadata": {
        "id": "_1LHmOwmBpH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# selenium 설치하기 \n",
        " - 필요한 드라이브 설치 "
      ],
      "metadata": {
        "id": "cSlUeicQCKyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "!pip install webdriver-manager\n",
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "I7pbx7deCAzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a3e5a6-40c2-408e-eb44-229fd5cb487b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [2 InRelease 14.2 kB/88.7 kB 16%] [Waitin\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [2 InRelease 14.2 kB/88.7 kB 16%] [Waitin\r0% [Connecting to archive.ubuntu.com] [2 InRelease 14.2 kB/88.7 kB 16%] [Waitin\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [2 InRelease 14.2 kB/88.\r0% [3 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (185.125.190.36\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [903 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [89.9 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,533 kB]\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,100 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,937 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,077 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,369 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,064 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,310 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,141 kB]\n",
            "Get:24 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 16.9 MB in 4s (3,990 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 90.4 MB of archives.\n",
            "After this operation, 306 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 103.0.5060.134-0ubuntu0.18.04.1 [1,160 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 103.0.5060.134-0ubuntu0.18.04.1 [79.0 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 103.0.5060.134-0ubuntu0.18.04.1 [5,043 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 103.0.5060.134-0ubuntu0.18.04.1 [5,202 kB]\n",
            "Fetched 90.4 MB in 6s (15.3 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155680 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_103.0.5060.134-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_103.0.5060.134-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (103.0.5060.134-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.4.0-py3-none-any.whl (985 kB)\n",
            "\u001b[K     |████████████████████████████████| 985 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 78.6 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 56.3 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.4.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.11 wsproto-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-3.8.3-py2.py3-none-any.whl (26 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from webdriver-manager) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from webdriver-manager) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2022.6.15)\n",
            "Installing collected packages: urllib3, python-dotenv, webdriver-manager\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.11\n",
            "    Uninstalling urllib3-1.26.11:\n",
            "      Successfully uninstalled urllib3-1.26.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.4.0 requires urllib3[secure,socks]~=1.26, but you have urllib3 1.25.11 which is incompatible.\u001b[0m\n",
            "Successfully installed python-dotenv-0.20.0 urllib3-1.25.11 webdriver-manager-3.8.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 멜론 TOP 50 크롤링 하기 \n"
      ],
      "metadata": {
        "id": "UKTwaqnCCkuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc1aKPb8Bklu",
        "outputId": "542553a7-45a5-490c-b577-f2060e136cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "음악 장르 입력(발라드, 댄스, 힙합, 알앤비, 인디, 록, 트로트, 포크) : 댄스\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Everyday 레몬티처럼 달콤한 입술 오직 너에게만 주는 나의 선물 아무도 모르게 네 품 안에서 너와 함께하고 싶어 oh 언제까지 I wanna be your babe ah ah I wanna be your girl 아침 햇살 가득한 오늘 널 만날 생각에 난 눈 뜨고 네가 예쁘다던 옷 입고 네 앞에 있는 나를 그려 너와 함께 걷는 이 길에 우리 발걸음 맞추다가 네 눈빛의 마법에 취해 너의 입술에 살짝 oh my Lips Everyday 레몬티처럼 달콤한 입술 오직 너에게만 주는 나의 선물 아무도 모르게 네 품 안에서 너와 함께하고 싶어 oh 언제까지 I wanna be your babe ah ah I wanna be your girl 너의 그늘 안에 날 숨긴 모든 게 멈춰버린 이 순간 밝게 비추는 저 햇살도 환하게 웃어 날 향하고 시원한 바람에 날 실어 나의 미소에 너를 실어 너의 사랑을 느끼면서 우리 둘만의 특별한 Kiss Time Everyday 레몬티처럼 달콤한 입술 오직 너에게만 주는 나의 선물 아무도 모르게 네 품 안에서 너와 함께하고 싶어 oh 언제까지 내 입술 위에 작은 떨림 부드러운 햇살 너의 느낌 가슴 속 깊이 너의 사랑이 꿈결처럼 다가와 Everyday 레몬티처럼 달콤한 입술 오직 너에게만 주는 나의 선물 아무도 모르게 네 품 안에서 너와 함께하고 싶어 oh 언제까지 Everyday 내 사랑 네게만 주고 싶어 언제나 곁에서 이렇게 영원히 Everyday 내 사랑 꿈속에서도 너와 함께하고 싶어 Forever with you I wanna be your babe ah ah I wanna be your girl']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import requests\n",
        "import bs4 \n",
        "# 멜론 장르음악\n",
        "while(True):\n",
        "    genre = input(\"음악 장르 입력(발라드, 댄스, 힙합, 알앤비, 인디, 록, 트로트, 포크) : \")\n",
        "    if genre == \"발라드\":\n",
        "        gnrCode = \"GN0100\"\n",
        "        break\n",
        "    elif genre == \"댄스\":\n",
        "        gnrCode = \"GN0200\"\n",
        "        break\n",
        "    elif genre == \"힙합\":\n",
        "        gnrCode = \"GN0300\"\n",
        "        break\n",
        "    elif genre == \"알앤비\":\n",
        "        gnrCode = \"GN0400\"\n",
        "        break\n",
        "    elif genre == \"인디\":\n",
        "        gnrCode = \"GN0500\"\n",
        "        break\n",
        "    elif genre == \"록\":\n",
        "        gnrCode = \"GN0600\"\n",
        "        break\n",
        "    elif genre == \"트로트\":\n",
        "        gnrCode = \"GN0700\"\n",
        "        break\n",
        "    elif genre == \"포크\":\n",
        "        gnrCode = \"GN0800\"\n",
        "        break\n",
        "    else:\n",
        "        print(\"장르 입력이 잘못 되었습니다. 다시 입력해주십시오.\")\n",
        "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gec'}\n",
        "for i in range(1, 52, 50):\n",
        "    Song_req = requests.get(f'https://www.melon.com/genre/song_list.htm?gnrCode={gnrCode}#params%5BgnrCode%5D={gnrCode}&params%5BdtlGnrCode%5D=&params%5BorderBy%5D=NEW&params%5BsteadyYn%5D=N&po=pageObj&startIndex={i}', headers = header)\n",
        "    Song_html = Song_req.text\n",
        "    Song = bs4.BeautifulSoup(Song_html)\n",
        "\n",
        "    Song_botten = Song.findAll('a')\n",
        "\n",
        "    Song_elements = Song.find_all('a', class_=\"btn button_icons type03 song_info\")\n",
        "\n",
        "    Song_list = str(Song_elements).split('</a>,')\n",
        "\n",
        "    Song_no_list = [Song_list[i].split(\"\\'\")[1] for i in range(len(Song_list))]\n",
        "\n",
        "    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko'}\n",
        "\n",
        "    Song = []\n",
        "\n",
        "\n",
        "    for Song_no in Song_no_list:\n",
        "    # 선택한 장르 송넘버로 해당 노래 페이지로 이동\n",
        "        req = requests.get(f'https://www.melon.com/song/detail.htm?songId={Song_no}', headers = header) \n",
        "\n",
        "    # bs4로 해당 페이지의 코드 긁어오기\n",
        "    Song_html = req.text\n",
        "    Song_lyrics = bs4.BeautifulSoup(Song_html)  \n",
        "\n",
        "    # 긁어온 코드 중에서 ('div', class_=\"lyric\")에 해당하는 부분 lyrics_elements에 담기\n",
        "    lyrics_elements = Song_lyrics.find_all('div', class_=\"lyric\") \n",
        "      # <br/>을 기준으로 str형으로 변형한 후 나누기\n",
        "    if len(lyrics_elements) == 0:\n",
        "        pass\n",
        "    elif len(lyrics_elements) == 1:\n",
        "        lyrics = str(lyrics_elements[0]).split('<br/>')\n",
        "        # 필요 없는 부분 제거\n",
        "        lyrics[0] = lyrics[0][81:]  # <div class=\"lyric\" id=\"d_video_summary\"><!-- height:auto; 로 변경시, 확장됨 -->\\r\\n\\t\\t\\t\\t\\t\\t\\t 제거\n",
        "        lyrics[-1] = lyrics[-1][:-13]   # \\r\\n\\t\\t\\t\\t\\t</div> 제거\n",
        "        while '' in lyrics:\n",
        "            lyrics.remove('')   # '' 제거\n",
        "        \n",
        "        \n",
        "        # Song 리스트 속에 리스트 형식으로 각각 노래 넣기\n",
        "        lyrics = ' '.join(lyrics)\n",
        "        Song.append(lyrics)\n",
        "\n",
        "Song"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Song)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H78n6EYjCvG8",
        "outputId": "134f3c07-ae43-4d3f-f791-cdd501b2003a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus= []\n",
        "\n",
        "for idx, sentence in enumerate(Song):\n",
        "    if len(sentence) == 0: continue           # 길이가 0인 문장 제외\n",
        "    if ('[' and ']') in sentence: continue    # 대괄호가 들어가는 문장 제외\n",
        "    \n",
        "    corpus.append(sentence)\n",
        "\n",
        "\n",
        "print(f'원래 데이터 크기 : {len(Song)}')\n",
        "print(f'공백인 문장, 대괄호 포함된 문장 제외한 데이터 크기 : {len(corpus)}')\n",
        "\n",
        "corpus = list(set(corpus))          # 중복불가한 set의 특성을 이용하여 중복되는 문장 제외 후 다시 리스트로 변환\n",
        "print(f'중복되는 문장 제외한 데이터 크기 : {len(corpus)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFkjzr_JCvke",
        "outputId": "8a7c54d5-5f70-4fcb-949c-1b7577b7843a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 데이터 크기 : 47\n",
            "공백인 문장, 대괄호 포함된 문장 제외한 데이터 크기 : 47\n",
            "중복되는 문장 제외한 데이터 크기 : 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob  #파일읽어오기\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import bs4 \n",
        "import requests"
      ],
      "metadata": {
        "id": "x9nNqhMGFJMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 정의\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()                     # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
        "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
        "    sentence = sentence.strip()                             # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>'             # 6\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "1Jpr-vj2EjxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tokenized = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    # 데이터 전처리\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus_tokenized.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10곡 확인\n",
        "corpus_tokenized[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8apihd5E64L",
        "outputId": "6d3a3fd1-c748-469b-a35a-7af90a259ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> 모두 한 공간에 넘쳐 승낙은 끄덕임 속에 멈춰 내 것이 곧 모두의 것 함께 나누며 거닐던 그 벗 웃고 눈물 흘릴 때도 나눌줄 알던 간간한 태도 소리없이 손 내밀어 서로 지켜준 각자의 mirror 벌써 그리워져 짧기만 했던 그 때 이젠 꿈이라는걸 알기에 입술만 깨물 뿐 보기 힘들어져 널부러진 공감대 이젠 기억속 미소 옆에 남아 흐르는 눈물 뿐 큰 눈망울은 여전히 멈출수 없는 시간의 journey 영원한 약속은 어려워 마침표는 모두를 넘어버려 나만이 불러야 할 노래들 난 많이 즐거웠다 느꼈어 만만히 봐왔던 그리움들 만반의 준비가 모자랐어 벌써 그리워져 짧기만 했던 그 때 이젠 꿈이라는걸 알기에 입술만 깨물 뿐 보기 힘들어져 널부러진 공감대 이젠 기억속 미소 옆에 남아 흐르는 눈물 뿐 이미 익숙해져 대면대면한 관계 이젠 침묵속 굳은 표정만을 가리는 그늘 뿐 <end>',\n",
              " '<start> kyt와함께 party 난리나지 모두 다 날지 이건 미쳐 다같이 노래하고 춤을 추는 곡 마음대로 하고 싶은 날 있어요 alright alright 그 날이 바로 오늘 이잖아요 rrrrr hey 눈 감으면 항상 그려왔던 낙원을 만들어요 편한 운동화에 트레이닝복 굿모닝 커피한잔 들고 baby 잔소린 오늘만은 노코멘 no coment 내 버려둬 날 좀 run away go away 저멀리 far away 사람들 시선을 저 멀리 i don t care 누구나 hey 함께야 hey 나를 hey 따라해 hey 아침이 올때까지 all way 잊어버려 cha 답답한 도시를 떠나버려 ha 모두 뒤로 하고 rrrrr hey 오늘밤엔 신나게 춤을 춰 춤을 춰 just go 달려볼까 cha 갑갑한 이 맘을 떨쳐볼까 ha 오늘만큼은 오늘만은 처음부터 처음부터 새롭게 시작해 다시 한번 시작해 빽가 간다 시원하게 부서지는 바다위에 파도 봐도봐도 아름다운 나는 너의 바보 오늘도 내일도 매일똑 같은 일상의 반복 i don t care amp it s ok 난 익숙해 그렇게 내가 너를 지킬게 약속해 나 you are my sunshine my only sunshine run away go away 저멀리 far away 사람들 시선을 저 멀리 i don t care 누구나 hey 함께야 hey 나를 hey 따라해 hey 아침이 올때까지 all way 잊어버려 cha 답답한 도시를 떠나버려 ha 모두 뒤로 하고 오늘밤엔 신나게 춤을 춰 춤을 춰 just go 달려볼까 cha 갑갑한 이 맘을 떨쳐볼까 ha 오늘만큼은 오늘만은 처음부터 처음부터 새롭게 시작해 다시 한번 시작해 아무도 내 마음 몰라주고 잘해보려고 해도 자꾸 실수하고 하루하루 남 눈치만 보며 살아가고 있잖아 근데 있잖아 다 괜찮아 시간이 지나면 다 잊잖아 그러니 괜찮아 넌 정말 괜찮아 just go let s go don t stop here we go now all way 잊어버려 cha 답답한 도시를 떠나버려 ha 모두 뒤로 하고 오늘밤엔 신나게 춤을 춰 춤을 춰 just go 달려볼까 cha 갑갑한 이 맘을 떨쳐볼까 ha 오늘만큼은 오늘만은 처음부터 처음부터 새롭게 시작해 다시 한번 시작해 <end>',\n",
              " '<start> are you ready alright 넌 어떤 냄샐 맡은 건지 이상한 눈초리 looking at me 뭐가 자꾸 궁금한 건지 가만있지 않는 너의 붉은 lips 난 사실 별로 상관없지 내 시야엔 네가 잘 안 보이니 너는 내 스타일이 아니니까 제발 관심 끄고 내 말 listen at me i m not your mystery 궁금해하지 마 넌 풀어보려 애쓰지 어려운 문제처럼 boy i m not your lady 자꾸 선 넘지 마 받아줄 생각이 전혀 없으니까 늘어만 가 too much attention 굳이 난 i don t say no 무슨 말을 바래 넌 어젯밤 누구와 어디서 반짝거렸는지 넌 상상도 못할 걸 난 좀 그래 바빠서 매일이 기억 못 해 수많은 밤 난 원하는 건 언제든 get it 눈치 따윈 안 봐 그냥 do what i like 뭐든 나 답게 with the right mind set 이게 딱 난데 혹시나 너 김 빠진 건 아닌지 상상의 나랠 펼치고 싶니 거기도 아마 너는 없겠지 어차피 주인공 나 한 명이니 다시 또 막을 내려야지 i m not your mystery 착각은 throw away boy i m not your lady 안 해도 되는 의심 같은 건 그만 늘어만 가 too much attention 굳이 난 i don t say no 무슨 말을 바래 넌 어젯밤 누구와 어디서 반짝거렸는지 넌 상상도 못 할 걸 난 좀 그래 바빠서 매일이 기억 못 해 수많은 밤 난 원하는 건 언제든 get it ah 작은 관심은 싫지 않아 착각이 싫을 뿐인 걸 조급함은 거둔 채로 이 노래나 더 즐기고 가 okay everybody listen up this goes out to all the boys and girls wanted to have a good time sing with me it is time to end this game 다시 나를 귀찮게 할 일 없게 늘어만 가 too much attention 굳이 난 i don t say no 무슨 말을 바래 넌 어젯밤 누구와 어디서 반짝거렸는지 넌 상상도 못할 걸 이제 그만 no more question 물어도 i know say no 무슨 말을 해도 넌 넌 날 이해할수 없어 예상할수 없어 나를 가질수 없어 boy <end>',\n",
              " '<start> 1 , 2 , 3 , 4 baby , got me looking so crazy 빠져버리는 daydream got me feeling you 너도 말해줄래 누가 내게 뭐라든 남들과는 달라 넌 maybe you could be the one 날 믿어봐 한번 i m not looking for just fun maybe i could be the one oh baby 예민하대 나 lately 너 없이는 매일 매일이 yeah 재미없어 어쩌지 i just want you call my phone right now i just wanna hear you re mine cause i know what you like boy you re my chemical hype boy 내 지난날들은 눈 뜨면 잊는 꿈 hype boy 너만 원해 hype boy 내가 전해 and we can go high 말해봐 yeah 느껴봐 mm mm take him to the sky you know i hype you boy 눈을 감아 말해봐 yeah 느껴봐 mm mm take him to the sky you know i hype you boy 잠에 들려고 잠에 들려 해도 네 생각에 또 새벽 세 시 uh oh 알려줄 거야 they can t have you no more 봐봐 여기 내 이름 써있다고 누가 내게 뭐라든 남들과는 달라 넌 maybe you could be the one 날 믿어봐 한번 i m not looking for just fun maybe i could be the one oh baby 예민하대 나 lately 너 없이는 매일 매일이 yeah 재미없어 어쩌지 i just want you call my phone right now i just wanna hear you re mine cause i know what you like boy you re my chemical hype boy 내 지난날들은 눈 뜨면 잊는 꿈 hype boy 너만 원해 hype boy 내가 전해 and we can go high 말해봐 yeah 느껴봐 mm mm take him to the sky you know i hype you boy 눈을 감아 말해봐 yeah 느껴봐 mm mm take him to the sky you know i hype you boy <end>',\n",
              " '<start> 언젠가 넌 이곳을 떠난다 해도 난 오늘처럼 너를 위해 아주 기쁜 춤을 출래 난 너의 눈빛이 닿을 때 춤을 출래 because you are my sunlight 지금 모든 게 영원하도록 눈부신 순간 우릴 기억해 i m in love with you 처음부터 마지막까지 너와 나 우리의 태양은 찬란하게 그 어떤 말도 필요 없어 이대로 눈 감아도 i remember it all so clearly 빠르게 흘러가는 음악에 손을 잡아주었던 그때처럼 그렇게 oh 예쁘게 웃어줄래 저기 노을빛 속에 내 맘을 담아볼래 눈물이 나기 전에 난 너의 눈빛이 닿을 때 춤을 출래 because you are my sunlight 지금 모든 게 영원하도록 눈부신 순간 우릴 기억해 i m in love with you 너무 아름다웠어 그치 하늘이 번져갈 때마다 생각나게 여기에 남겨놓고 싶어 너를 바라보면서 i m saying that i ll always love you 내가 주저앉으면 어디든 나란히 누워주던 너와 함께라는 말이 낯설지 않아졌네 사랑은 숨이 차게 저 멀리 달아난대 더 슬퍼지지 않게 난 너의 눈빛이 닿을 때 춤을 출래 because you are my sunlight 지금 모든 게 영원하도록 눈부신 순간 우릴 기억해 i m in love with you 언젠가 넌 이곳을 떠난다 해도 난 오늘처럼 너를 위해 아주 기쁜 춤을 출래 the sun will go down but we will rise it s a good thing because you are my sunlight let s get on with it it s time for dance my day is made i m in how about you 캄캄한 밤이 온다 해도 너와 함께라면 간직할 highlight 이게 운명일까 우리 함께 숨 쉬는 매일을 잊을 수 없어 i m in love with you i m in love with you <end>',\n",
              " '<start> forever 1 it s love it s love we re not stopping 네가 머문 이 세상이 더 아름다운 건 겁 없이 외치던 말 사랑해 너를 영원하기에 you and i 터지는 눈물이 말하잖아 난 그냥 전부 던진 거야 아무런 망설임 따위도 멋대로 끌렸던 그대로 oh my baby 달려가 안을게 i love 너의 모든 것 , 내 전부인 너 우리는 영원 we are one 전율 속에 뜨거운 그 맘을 던져 just like a love bomb we are one girls , we are forever yeah we are , we re still forever 1 it s now or never we keep on , we re still forever 1 yeah we re forever yeah we are , we re still forever 1 날 꼭 안아 절대 놓치지 마 가슴이 뛰잖아 다시는 아파하지 마 너의 마음을 우린 다 알아 다 알아 내 곁에 있어 줘 이 순간도 마지막처럼 you know your love is crazy 항상 그랬지 oh my baby 사랑해 기억해 i love 너의 모든 것 , 내 전부인 너 우리는 영원 we are one 전율 속에 뜨거운 그 맘을 던져 just like a love bomb we are one girls , we are forever yeah we are , we re still forever 1 it s now or never we keep on , we re still forever 1 yeah we re forever yeah we are , we re still forever 1 언제나 너의 곁에 있고 싶은데 널 생각하면 강해져 there s no one like you , no one like you 우리 꼭 영원하자 우리 머문 이 세상에서 네게 말했어 다시 태어나도 널 사랑할게 cause we are the one i love 너의 ooh i love 나의 모든 것 내 전부인 너 전부인 너 우리는 영원 ooh we are one 전율 속에 그 전율 속에 뜨거운 그 맘을 던져 just like a love bomb just like a love bomb we are one girls , we are forever yeah we are , we re still forever 1 it s now or never we keep on , we re still forever 1 yeah we re forever yeah we are , we re still forever 1 <end>',\n",
              " '<start> 가끔씩은 정신없고 하루종일 엉킬 때 있어 바쁜 시간 뒤에 만나줘 네 입술 눈이 닿을때 그 느낌대로 맞춰도 될까 i m fallin in love like 파도 위 surf vibe i spinnin around you 우주안에 넌 딱 유일 해 상자속에서 꺼내준건 너 뿐이야 그대로 날 hey keep movin on 자유를 느껴 시원한 바람 사이에 dance smooth criminal 볼륨을 올려 whippin in the car right now 아쉬움이 스친 다면 걱정마 이제 괜찮아 yes i do 더 천천히 okay feel the rhythm it s like yeah after then come and stay here 보낼래 나와 더 우리 둘 two of us yeah yeah 그냥 두지 않을게 after 9 to 6 oops i gotta go 잠못드는 시간이 좀 많아서 이만 bye bye where you wanna go 분위기에 취해 올라 오늘밤 올라가 divin 겁먹지말고 미리 relax your mind yeah 숨쉬고 있어 난 i don t feel that wastin time for ya all day 똑같은 모습이라해도 호흡해 깊게 너의 작은 숨 벅 차오를 때까지 hey keep movin on 자유를 느껴 시원한 바람 사이에 dance smooth criminal 볼륨을 올려 whippin in the car right now 아쉬움이 스친 다면 걱정마 이제 괜찮아 yes i do 더 천천히 okay feel the rhythm it s like yeah after then come and stay here 보낼래 나와 더 우리 둘 two of us yeah oh yeah 그냥 두지 않을게 after 9 to 6 <end>',\n",
              " '<start> all i wanna do is just change your life 지루한 일상에서 벗어나 maybe worried for a while 내게 너를 맡겨줘 yeah 싫다는 말은 마 we got a lot of time yeah the very night 저 달은 널 비추는 light baby going down bass yeah 오늘 이 순간 we re gonna party all night on stage yeah i ll show you let s ride 춤을 춰줘 all night dance like we re nineteen 음악에 맞춰 move the right girl you are mine let me show your body 우리 둘이서 party all night you you wanna come with me 챙길것도 없어 이미 우린 done with it 다신 돌아오지 않을 너의 fantasy do we get alright yeah 피폐해진 너의 삶에 break up 더 이상의 고민은 내일로 분위기에 맞춰서 탱고 shall we dance alright the very night 저 달은 널 비추는 light baby going down bass yeah 오늘 이 순간 we re gonna party all night on stage yeah i ll show you let s ride 춤을 춰줘 all night dance like we re nineteen 음악에 맞춰 move the right girl you are mine let me show your body 우리 둘이서 party all night don t you wake up we need more make another fiction 지금이야 만들어가 이 순간 don t you wake up we need more make another fiction 지금이야 we re gonna be alright let s ride 춤을 춰줘 all night dance like we re nineteen 음악에 맞춰 move the right girl you are mine let me show your body 우리 둘이서 party all night let s ride 춤을 춰줘 all night dance like we re nineteen 음악에 맞춰 move the right girl you are mine let me show your body 예 우리 둘이서 party all night <end>',\n",
              " '<start> 긴 밤에도 외롭지 않죠 내 술잔에 비치는 웃고 있는 you re my star 온 세상이 환해진단 걸 그대는 알까요 어떤 별보다 빛나 나 지금 혹시 꿈을 꾸는 걸까 자꾸만 두 발이 붕붕 뜨는 게 쌉쌀한 달달한 아찔한 그대 눈을 뗄 수 없죠 꽃처럼 꿀처럼 달콤한 그대 살짝 또 웃어주면 넘어 나 넘어가요 톡하면 넘어가죠 oh 어떡해 큰일 나 큰일났죠 한 모금 두 모금 세 모금 푹 빠져버린 걸요 이 밤이 더 길면 좋겠죠 잠 한숨을 못 자도 그댈 볼 수 있다면 눈 비비고 다시 볼 만큼 눈부신 그대는 어떤 별보다 빛나 나 지금 혹시 이상해 보일까 자꾸만 실없이 웃고 있는 게 쌉쌀한 달달한 아찔한 그대 눈을 뗄 수 없죠 꽃처럼 꿀처럼 달콤한 그대 살짝 또 웃어주면 넘어 나 넘어가요 톡하면 넘어가죠 oh 어떡해 큰일 나 큰일났죠 한 모금 두 모금 세 모금 푹 빠져버린 걸요 love love love 눈을 뜨고 나면 슬플지 몰라 사라질지 몰라 love love love 날이 밝아오면 두통뿐일지 몰라 그렇대도 난 이 순간에 말할래 그댈 사랑한다고 밥보다 술보다 사랑해요 난 나의 요정 그댈 한 잔 더 두 잔 더 아찔해요 난 아름다운 밤이네요 그댄 더 아름답죠 밤하늘 달빛처럼 oh 난 몰라 어쩜 이리 달까요 오늘도 내일도 모레도 당신의 눈에 건배 <end>',\n",
              " '<start> 왜 이래 왜 이래 정말 왜 이러니 오늘따라 왜 이리 망가지는거니 예쁜 내 원피스 whit 번지는 red hot dot 쏟아지는 black drink 모두 놀라잖아 빙글 머리가 핑핑 숨고만 싶은걸 uh uh 우당탕탕 심장이 뚝뚝 눈물이 흘러 나를 데려가 oh follow away 호호호 i wanna be free 초코 마카롱 날 촉촉하게 감싸안아줘 now 가나슈 쇼콜라 나쁜 기억 모두 안녕 sugar sugar reset all right all right i m so all right all right 이렇게 nice한 기분 정말 설레는걸 like a flying flying 기분좋은 아잉 아잉 이렇게 날아갈 것 처럼 날 따라 춤춰봐 turn turn on the switch 나나나처럼 나나나처럼 turn turn on the switch 나나나처럼 나나나처럼 아이쿠 방글 웃음이 나와 춤추고 싶은걸 댄스 댄스 우당탕탕 신나게 똑똑 노크를 하면 누군가 hey with me 나올까 i wanna have fun 초코 마카롱 날 촉촉하게 감싸안아줘 now 가나슈 쇼콜라 나쁜 기억 모두 안녕 sugar sugar reset all right all right i m so all right all right 이렇게 nice한 기분 정말 설레는걸 like a flying flying 기분좋은 아잉 아잉 이렇게 날아갈 것 처럼 날 따라 춤춰봐 yeah alright turn on the switch 마치 바람에 실리는것 처럼 갇혀있던 숨막히던 내가 아냐 마치 물길이 흐르는것 처럼 자유롭게 날고 있는 나를 봐봐 i feel like i have the whole new world 다시 그때로 돌아가지 않아 all right all right i m so all right all right 이렇게 nice한 기분 정말 설레는걸 like a flying flying 기분좋은 아잉 아잉 이렇게 날아갈 것 처럼 날 따라 춤춰봐 turn turn on the switch 나나나처럼 나나나처럼 turn turn on the switch 나나나처럼 나나나처럼 <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 형태소분석기 설치"
      ],
      "metadata": {
        "id": "ISC9am0VFabX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
      ],
      "metadata": {
        "id": "nfRaDlgFE8-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 형태소 분석기\n",
        "- morphs( text )\n",
        "  - 텍스트에서 형태소를 반환한다\n",
        " \n",
        "- nouns( text )\n",
        "  - 텍스트에서 명사를 반환한다\n",
        " \n",
        "- pos( text )\n",
        "  - 텍스트에서 품사 정보를 부착하여 반환한다\n",
        "- sentences( text )\n",
        "  - 텍스트에서 문장별로 변환한다\n",
        "- Hannanum: 한나눔. KAIST Semantic Web Research Center 개발.\n",
        "\n",
        "  - http://semanticweb.kaist.ac.kr/hannanum/\n",
        "\n",
        "- Kkma: 꼬꼬마. 서울대학교 IDS(Intelligent Data Systems) 연구실 개발.\n",
        "\n",
        "  - http://kkma.snu.ac.kr/\n",
        "\n",
        "- Komoran: 코모란. Shineware에서 개발.\n",
        "\n",
        "  - https://github.com/shin285/KOMORAN\n",
        "\n",
        "- Mecab: 메카브. 일본어용 형태소 분석기를 한국어를 사용할 수 있도록 수정.\n",
        "\n",
        "  - https://bitbucket.org/eunjeon/mecab-ko\n",
        "\n",
        "- Open Korean Text: 오픈 소스 한국어 분석기. 과거 트위터 형태소 분석기.\n",
        "\n",
        "  - https://github.com/open-korean-text/open-korean-text"
      ],
      "metadata": {
        "id": "sDvlN_inc0us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "\n",
        "# kkm = Kkma()\n",
        "# kom = Komoran()\n",
        "# mec = Mecab()\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "DzNRSc2VFc4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# okt 토큰화\n",
        "ko_tokenized = []\n",
        "\n",
        "for c in corpus_tokenized:\n",
        "   ko_tokenized.append(okt.morphs(c)) # morphs => 텍스트에서 형태소를 반환한다"
      ],
      "metadata": {
        "id": "qxS8usXpFlQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 곡의 시작과 끝을 <start>와 <end>로 표시\n",
        "song_list = []\n",
        "for song in ko_tokenized:\n",
        "  song.insert(0, '<start>')\n",
        "  song.append('<end>')\n",
        "  song_list.append(song)"
      ],
      "metadata": {
        "id": "7v4VPPpZFnjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###토큰화2. 단어장 만들기 (tokenizer)"
      ],
      "metadata": {
        "id": "IagyqP-4F_nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=15000,         # 15000개 단어를 기억할 수 있음\n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"        # 포함되지 않는 단어는 <unk> 으로 표현\n",
        "    )\n",
        "    \n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  # 토큰 15개 초과 제외\n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer"
      ],
      "metadata": {
        "id": "rjrmohLJGAJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor, tokenizer = tokenize(song_list)"
      ],
      "metadata": {
        "id": "xLZgdBh7GBgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어장의 1~10번째 단어 확인해보기\n",
        "\n",
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9nvtt1GD66",
        "outputId": "098307e5-c5bc-45d5-b82e-49b9307951ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : 을\n",
            "3 : 이\n",
            "4 : you\n",
            "5 : 너\n",
            "6 : i\n",
            "7 : 에\n",
            "8 : the\n",
            "9 : 가\n",
            "10 : 내\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = tensor[:, :-1] # 소스문장 생성 (마지막 단어 1개 빼고 다 가져오기)\n",
        "\n",
        "tgt_input = tensor[:, 1:]  # 타켓문장 생성 (첫번째 단어 1개 빼고 다 가져오기)\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgxkK9EvGGtG",
        "outputId": "e0720057-c8eb-4fb2-98c0-88e45e6085e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1329    3  242 1330   38 1331  440  441 1332  903  243   15   44   16]\n",
            "[   3  242 1330   38 1331  440  441 1332  903  243   15   44   16   48]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소스문장 인덱스 -> 단어로 변환\n",
        "for idx in src_input[0]:\n",
        "  print(tokenizer.index_word[idx], end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiOzd9HfGIYT",
        "outputId": "9723a318-ca7c-42e7-c9f1-f21a26cc8c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "관계 이 젠 침묵 속 굳은 표정 만을 가리는 그늘 뿐 < end > "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 타켓문장 인덱스 -> 단어로 변환\n",
        "for idx in tgt_input[0]:\n",
        "  print(tokenizer.index_word[idx], end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv_taKB6GI7q",
        "outputId": "1439707f-f6c6-4d83-d0c9-ca9d84261293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 젠 침묵 속 굳은 표정 만을 가리는 그늘 뿐 < end > <end> "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "LCBGg2KWGMSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,       # 데이트셋 비율\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=121)     # 결과를 일정하게 보여주기위해 지정"
      ],
      "metadata": {
        "id": "5s8y0eL-GJ_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YPRBAEhGPGr",
        "outputId": "bcfa4e65-8c40-4982-d22d-d0d3380f8ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Train: (37, 14)\n",
            "Target Train: (37, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###모델학습"
      ],
      "metadata": {
        "id": "btQMyiVIGSt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding() - 단어를 밀집 벡터로 만드는 역할\n",
        "#             - 인공 신경망 용어로는 임베딩 층(embedding layer)을 만드는 역할을 합니다. \n",
        "#             - Embedding()은 정수 인코딩이 된 단어들을 입력을 받아서 임베딩을 수행합니다.\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        \n",
        "        self.embedding = Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "LPpn0SpbGSJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.num_words + 1\n",
        "embedding_size = 700\n",
        "hidden_size = 350\n",
        "# drop_rate = 0.2\n",
        "model = TextGenerator(vocab_size, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "7vD4sCnsGWnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.nn_ops import dropout\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train, epochs=200, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdDb7dDbGX8S",
        "outputId": "502ecf6c-1d88-485f-dcfc-42f290a84a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 14s 854ms/step - loss: 9.6158\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 2s 604ms/step - loss: 9.6041\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 2s 618ms/step - loss: 9.5617\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 2s 542ms/step - loss: 9.3393\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 2s 604ms/step - loss: 8.5604\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 2s 620ms/step - loss: 7.5430\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 2s 573ms/step - loss: 6.6037\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 2s 466ms/step - loss: 5.9477\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 1s 374ms/step - loss: 5.5183\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 1s 409ms/step - loss: 5.2979\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 1s 372ms/step - loss: 5.1373\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 1s 356ms/step - loss: 5.0832\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 4.9928\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 4.9599\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 4.9058\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 4.8801\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 4.8464\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.8060\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 1s 374ms/step - loss: 4.7874\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 1s 374ms/step - loss: 4.7685\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 4.7520\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 4.7358\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 4.7411\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 1s 377ms/step - loss: 4.7348\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 4.6934\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 4.6967\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 4.6484\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 4.6721\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 1s 349ms/step - loss: 4.6598\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 4.6832\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 1s 379ms/step - loss: 4.6455\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 1s 379ms/step - loss: 4.6354\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 1s 369ms/step - loss: 4.6516\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.6698\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.6418\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 4.6556\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 4.6482\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 1s 372ms/step - loss: 4.6449\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 4.6427\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.6331\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 4.6447\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 1s 380ms/step - loss: 4.6434\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 1s 382ms/step - loss: 4.6417\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 1s 390ms/step - loss: 4.6188\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 4.6450\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.6288\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 1s 383ms/step - loss: 4.6450\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 1s 374ms/step - loss: 4.6354\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 1s 353ms/step - loss: 4.6207\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 1s 357ms/step - loss: 4.6517\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 4.6122\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 1s 369ms/step - loss: 4.6439\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 1s 376ms/step - loss: 4.6140\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 1s 409ms/step - loss: 4.6127\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.6213\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 1s 356ms/step - loss: 4.6169\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 4.6013\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 1s 352ms/step - loss: 4.6156\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.5852\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 4.5744\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 4.5603\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 4.5766\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 4.5862\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 1s 361ms/step - loss: 4.5461\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 1s 583ms/step - loss: 4.5458\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 2s 546ms/step - loss: 4.5572\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 1s 353ms/step - loss: 4.5406\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 4.5378\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 1s 354ms/step - loss: 4.5222\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 4.5153\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 1s 366ms/step - loss: 4.5089\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 4.5124\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.4899\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 1s 393ms/step - loss: 4.4688\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 4.4440\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 4.4418\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.4143\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 4.4112\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 1s 393ms/step - loss: 4.4198\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 4.3944\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 1s 354ms/step - loss: 4.3729\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 4.3487\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 1s 382ms/step - loss: 4.3720\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 4.3551\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 4.3690\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 1s 377ms/step - loss: 4.3035\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 4.3926\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 1s 350ms/step - loss: 4.3605\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.3579\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 4.2888\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 4.3300\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 1s 398ms/step - loss: 4.2562\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 4.2790\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 4.2261\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 4.2545\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 1s 354ms/step - loss: 4.2474\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 4.2510\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 4.1944\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 4.2119\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 1s 356ms/step - loss: 4.1777\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.1651\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 1s 352ms/step - loss: 4.2240\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.2212\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 1s 369ms/step - loss: 4.2002\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 1s 372ms/step - loss: 4.1239\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 1s 382ms/step - loss: 4.1735\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 4.1201\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 4.1316\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.1007\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.1096\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.1093\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 1s 357ms/step - loss: 4.0853\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 1s 361ms/step - loss: 4.0824\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 1s 361ms/step - loss: 4.0494\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 4.0610\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 1s 377ms/step - loss: 4.0451\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 4.0505\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 4.0293\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 4.0078\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 1s 379ms/step - loss: 4.0124\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 3.9897\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 1s 357ms/step - loss: 4.0013\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 4.0081\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.0437\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 3.9646\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 4.0036\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 4.0554\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 3.9665\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 1s 366ms/step - loss: 4.0871\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 1s 369ms/step - loss: 4.0021\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 4.0258\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 1s 376ms/step - loss: 3.9665\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 4.0337\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 1s 356ms/step - loss: 3.9320\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 3.9672\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 1s 377ms/step - loss: 3.9181\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 1s 353ms/step - loss: 3.9540\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 3.9111\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 2s 603ms/step - loss: 3.9192\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 2s 366ms/step - loss: 3.8941\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 3.8976\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 1s 376ms/step - loss: 3.8851\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 1s 372ms/step - loss: 3.8614\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 3.8695\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 3.8978\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 3.8505\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 1s 381ms/step - loss: 3.8550\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 3.8674\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 1s 358ms/step - loss: 3.8260\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 3.8698\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 3.8438\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 3.8238\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 3.8687\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 1s 356ms/step - loss: 3.8103\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 3.8123\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 3.8022\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 1s 361ms/step - loss: 3.8058\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 3.7964\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 3.7745\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 3.7660\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 1s 351ms/step - loss: 3.7585\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 1s 357ms/step - loss: 3.7792\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 1s 353ms/step - loss: 3.7785\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 1s 351ms/step - loss: 3.8055\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 3.9289\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 3.8041\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 3.9052\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 3.7796\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 1s 357ms/step - loss: 3.8315\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 3.7471\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 3.7907\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 3.7158\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 1s 362ms/step - loss: 3.7484\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 3.7101\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 1s 390ms/step - loss: 3.7259\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 1s 366ms/step - loss: 3.6988\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 3.7166\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 1s 373ms/step - loss: 3.6907\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 3.6938\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 3.6822\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 3.6724\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 3.6570\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 3.6436\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 1s 401ms/step - loss: 3.6474\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 1s 361ms/step - loss: 3.6371\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 3.6328\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 1s 372ms/step - loss: 3.6530\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 1s 366ms/step - loss: 3.6208\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 3.6273\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 3.6216\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 3.6285\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 3.5991\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 1s 379ms/step - loss: 3.6011\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 1s 353ms/step - loss: 3.6051\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 3.5986\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 1s 356ms/step - loss: 3.5795\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 1s 374ms/step - loss: 3.5687\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 3.5795\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 1s 369ms/step - loss: 3.5764\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 1s 356ms/step - loss: 3.5540\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4863737f90>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 생성 함수\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
        "    while True:\n",
        "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
        "\n",
        "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "\n",
        "    generated = \"\"\n",
        "        # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        if word_index == 0:\n",
        "            continue\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated #최종적으로 모델이 생성한 자연어 문장"
      ],
      "metadata": {
        "id": "titmK4qGGaC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링한 댄스 50곡으로 200번 학습한 결과, okt 사용, <start>, <end> 추가 \n",
        "\n",
        "print('예시1.', generate_text(model, tokenizer, init_sentence='힘내', max_len=50))\n",
        "print('예시2.', generate_text(model, tokenizer, init_sentence='슬픔', max_len=50))\n",
        "print('예시3.', generate_text(model, tokenizer, init_sentence='비가 내려', max_len=50))\n",
        "print('예시4.', generate_text(model, tokenizer, init_sentence='i love', max_len=50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLFgsWYEGc7F",
        "outputId": "616b8b52-1fbb-4f69-c05b-624de2e1c07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시1. <unk> 도 crazy 도 와 i i hmm hmm < < < > <end> \n",
            "예시2. <unk> 도 crazy 도 와 i i hmm hmm < < < > <end> \n",
            "예시3. <unk> 내려 을 도 와 i hmm hmm hmm < < end <end> \n",
            "예시4. i love 도 와 i i i hmm hmm < < < > <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델링 팀의 성공적인 학습방법"
      ],
      "metadata": {
        "id": "8h4d6UCUdVM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 개요\n",
        "- 프로젝트 주제: RNN을 활용한 장르별 노래 가사 생성 시스템\n",
        "- 데이터 출처: https://github.com/sohyeon98720/NLP/tree/master/KoGPT2-finetuning_lyrics/dataset\n",
        "- 2011~2019년 발라드 곡 474개를 학습데이터로 활용함\n",
        "- 한국어 형태소 분석기 konlpy의 Okt 사용하여 토큰화\n",
        "- LSTM 모델 적용"
      ],
      "metadata": {
        "id": "ZiPL_ihteIAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL5ZIFLgzrlf"
      },
      "outputs": [],
      "source": [
        "import glob  #파일읽어오기\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece #토크나이저 인스톨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdzWLwT-6Z6e",
        "outputId": "2132c9d5-0738-4002-aa27-a42fc1221b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_corpus = []\n",
        "with open(\"/content/2011-2019_발라드.tsv\", \"r\") as f:\n",
        "    raw = f.read().splitlines()  #문장단위 나눔\n",
        "    raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus)) # 곡의 개수\n",
        "print(\"Examples:\\n\", raw_corpus[:10]) #  곡 10개 가사 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Nf07yy1eBt",
        "outputId": "1818d66f-399c-480d-b80c-57dcbd772613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 474\n",
            "Examples:\n",
            " ['lyrics', '언젠간 이 눈물이 멈추길 언젠간 이 어둠이 걷히고 따스한 햇살이 이 눈물을 말려주길 지친 내 모습이 조금씩 지겨워지는 걸 느끼면 다 버리고 싶죠 힘들게 지켜오던 꿈을 가진 것보다는 부족한 것이 너무나도 많은 게 느껴질 때마다 다리에 힘이 풀려서 난 주저앉죠 언젠간 이 눈물이 멈추길 언젠간 이 어둠이 걷히고 따스한 햇살이 이 눈물을 말려주길 괜찮을 거라고 내 스스로를 위로하며 버티는 하루하루가 날 조금씩 두렵게 만들고 나를 믿으라고 말하면서도 믿지 못하는 나는 이제 얼마나 더 오래 버틸 수 있을 지 모르겠어요 기다리면 언젠간 오겠지 밤이 길어도 해는 뜨듯이 아픈 내 가슴도 언젠간 다 낫겠지 날 이젠 도와주길 하늘이 제발 도와주길 나 혼자서만 이겨내기가 점점 더 자신이 없어져요 언젠간 이 눈물이 멈추길 언젠간 이 어둠이 걷히고 따스한 햇살이 이 눈물을 말려주길 기다리면 언젠가 오겠지 밤이 길어도 해는 뜨듯이 이 아픈 내 가슴도 언젠간 다 낫겠지 언젠간 언젠간', '차가운 가슴이 어느새 조금씩 녹아 내렸나 봐 니가 들어왔어 그리고 나도 몰래 내 가슴을 채웠어 언제부터인가 집에 돌아오면 너를 떠올리고 있는 내 모습을 보면서 내 맘 속에 니가 있는 걸 알았어 Maybe youre the one Maybe 어쩌면 어쩌면 니가 내가 기다린 반쪽인건지 Maybe it is true 언제나 너무 가까이 있어서 몰랐었나봐 Baby Im in love with you 처음엔 몰랐어 내가 널 이렇게 떠올리게 될 줄 사랑하게 될 줄 니 맘도 제발 이런 내 마음과 같기를 Maybe youre the one Maybe 어쩌면 어쩌면 니가 내가 기다린 반쪽인건지 Maybe it is true 언제나 너무 가까이 있어서 몰랐었나봐 Baby Im in love with you 너무 늦진 않았길 이제야 깨달은 내 맘 받아주길 늦게 알았지만 이제야 알았지만 이 마음은 절대 흔들리지 않아 Maybe youre the one Maybe 어쩌면 어쩌면 니가 내가 기다린 반쪽인건지 Maybe it is true 언제나 너무 가까이 있어서 몰랐었나봐 Baby Im in love with you Baby Im in love with you Baby Im in love with you Baby Im in love with you Baby Im in love with you In love', '맨처음 너를 보던 날 수줍기만 하던 너의 맑은 미소도 오늘이 지나면 가까워 질거야 매일 설레는 기대를 해 무슨 말을 건네 볼까 어떻게 하면 네가 웃어줄까 손을 건네보다 어색해질까봐 멋쩍은 웃음만 웃어봐 우리 서로 반말하는 사이가 되기를 아직 조금 서투르고 어색한데도 고마워요 라는 말투 대신 좀 더 친하게 말을 해줄래 우리 서로 반말하는 사이가 될거야 한걸음씩 천천히 다가와 이젠 내 두눈을 바라보며 말을 해줄래 널 사랑해 너와의 손을 잡던날 심장이 멈춘듯한 기분들에 무슨말 했는지 기억조차 안나 마냥 설레는 기분인걸 우리 서로 반말하는 사이가 되기를 아직 조금 서투르고 어색한데도 고마워요 라는 말투 대신 좀 더 친하게 말을 해줄래 우리 서로 반말하는 사이가 될거야 한걸음씩 천천히 다가와 이젠 내 두눈을 바라보며 말을 해줄래 널 사랑해 우리 서로 사랑하는 사이가 되기를 잡은 두손 영원히 놓지 않을꺼야 바라보는 너의 눈빛속에 행복한 미소만 있길 바래 우리 서로 사랑하는 사이가 될꺼야 아껴주고 편히 기대면 돼 너를 보는 나의 두 눈빛이 말하고 있어 널 사랑해', '겨울에 태어난 아름다운 당신은 눈 처럼 깨끗한 나만의 당신 겨울에 태어난 사랑스런 당신은 눈처럼 맑은 나만의 당신 하지만 봄 여름과 가을 겨울 언제나 맑고 깨끗해 겨울에 태어난 아름다운 당신은 눈처럼 깨끗한 나만의 당신 하지만 봄 여름과 가을 겨울 언제나 맑고 깨끗해 겨울에 태어난 아름다운 당신은 눈처럼 깨끗한 나만의 당신 생일 축하합니다 생일축하합니다 생일 축하합니다 당신의 생일을 Happy birthday to you. Happy birthday to you. Happy birthday to you. Happy birthday to you. Happy birthday to you. Happy birthday to you. Happy birthday to you. Happy birthday to you.', '거짓말 날 위해 하는 말 혼잣말 버릇이 된 이말 괜찮아 질 거야 누구나 겪는거야 이런 하루가 반복되고 하루하루 일년인 것 같아 너 없는 오늘이 하루하루 힘들 것만 같아 숨쉬는 것 조차 너의 흔적들이 아직 남아 혼자 너무나도 아파 I cant let it go 마음은 움직일 수있어 이별도 되돌릴 수있어 기다려 줄게 나 이해해 줄게 다 널 찾는 하루 반복되고 하루하루 일년인 것 같아 너 없는 오늘이 하루하루 힘들 것만 같아 숨쉬는 것 조차 너의 흔적들이 아직 남아 혼자 너무나도 아파 I cant let it go 하루하루 없던 일처럼 돌릴 수 없니 텅 빈 내방을 채울 수 없이 흘러버린 추억들은 잡을 수 없어 하루하루 아플 것만 같아 하루하루 죽을 만큼 아파 너 없는 오늘이 하루하루 멈출 것만 같아 혼자인 세상이 우리 추억들이 아직 남아 가슴 깊이 파고 들어 I cant let it go 라라라라 Ill let u go 라라라라 Ill let u go 라라라라 하루하루 라라라라 하루하루', '매번 마주칠 때마다 니가 웃어줄 때마다 조금씩 내 안에 조금씩 널 향한 마음이 자라더니 이제는 널 생각하면 니 모습을 떠올리면 자꾸만 두근대는 내 심장은 멈출줄 몰라 더 이상 감출 수 없는 내맘 전부 다 주고 싶어 항상 곁에서 널 사랑 하면 안될까 누구보다 너를 지키고 싶은 맘 받아주면 안될까 너의 맘속에 내가 있으면 안될까 너에게 가장 소중한 사람이 되고 싶어 어서 내 맘을 받아줘 매일매일 커져가는 널 향한 사랑 때문에 난 정말 하루 종일 아무것도 할 수가없어 이제는 숨길 수 없는 내 맘 너로 가득 차버렸어 항상 곁에서 널 사랑 하면 안될까 누구보다 너를 지키고 싶은 맘 받아주면 안될까 너의 맘속에 내가 있으면 안될까 너에게 가장 소중한 사람이 되고 싶어 어서 내 맘을 받아줘 이런 내 마음을 알아줘 알겠다고 대답해줘 항상 곁에서 널 사랑 하면 안될까 누구보다 너를 지키고 싶은 맘 받아주면 안될까 너의 맘속에 내가 있으면 안될까 너에게 가장 소중한 사람이 되고 싶어 어서 내 맘을 받아줘 내 맘을 알아줘', '사랑해요 지금 곁에 없지만 그대가 있어 이 세상이 난 행복한걸요 그대 잊지마요 세상 어디에서도 내 사랑을 기억해줘요 사랑해요 그댈 사랑해요 그대 때문에 이렇게 그대 때문에 이렇게 눈물이나 다신 그댈 볼수 없어도 그래도 나는 괜찮아요 그댈 사랑하니까 우리 매일 걷던 익숙한 이 거리도 나는 오늘도 낯설게만 또 느껴지네요 그대 없는 동안 많은게 변했어도 내 사랑만 멈춰서 있죠 사랑해요 그댈 사랑해요 그대 때문에 이렇게 그대 때문에 이렇게 눈물이나 다신 그댈 볼수 없어도 그래도 나는 괜찮아요 그댈 사랑하니까 미안해요 내가 미안해요 나 때문에 힘들었죠 나 때문에 아팠겠죠 바보처럼 이제서야 알것 같아요 가슴이 원하는 한 사람 바로 그대이니까 우~ 사랑해요 영원히', '사랑을 믿지 않았지 오늘이 오기 전엔 그래서 가능했나봐 널 떠날 수 있었나봐 미련하기는 또 최고라 아픈지도 몰랐어 가슴이 텅 빈 것 같아 눈물이 자꾸 흐르다 니가 보고 싶어 죽을 것 같아 이제야 알았어 넌 내꺼중에 최고 내 삶의 모든 것 중에 최고 눈이 멀었었나봐 미쳤나봐 왜 너를 못 알아봐 나 따위가 뭐라고 감히 너를 떠나 살 수 있다고 내겐 너무 과분한 사람이란 걸 이제야 알았어 넌 내꺼중에 최고 잘난 것 하나 없는데 무슨 자신감으로 그렇게 널 대한건지 널 차버릴 수 있는지 자상하기는 또 최고라 화 한번 내지 않고 오직 나만 사랑해준 자기보다 더 아껴준 내겐 너무 과분한 그 사람이 이제야 그리워 넌 내꺼중에 최고 내 삶의 모든 것 중에 최고 눈이 멀었었나봐 미쳤나봐 왜 너를 못 알아봐 나 따위가 뭐라고 감히 너를 떠나 살 수 있다고 내겐 너무 과분한 사람이란 걸 이제야 알았어 내겐 자격따윈 없지만 니 곁에 돌아가겠다는 말은 뻔뻔하지만 한 번 실수한 만큼 더 잘 할 수도 있어 이런 날 믿고 다시 받아주겠니 넌 내꺼중에 최고 내 삶의 모든 것 중에 최고 눈이 멀었었나봐 미쳤나봐 왜 너를 못 알아봐 나 따위가 뭐라고 감히 너를 떠나 살 수 있다고 내겐 너무 과분한 사람이란 걸 이제야 알았어 넌 내꺼중에 최고', '\"정말 넌 다 잊었더라 반갑게 날 보는 너의 얼굴 보니 그제야 어렴풋이 아파오더라 새 살 차오르지 못한 상처가 눈물은 흐르질 않더라 이별이라 하는 게 대단치도 못해서 이렇게 보잘것없어서 좋은 이별이란 거, 결국 세상엔 없는 일이라는 걸 알았다면 그때 차라리 다 울어둘 걸 그때 이미 나라는 건 네겐 끝이었다는 건 나만 몰랐었던 이야기 사랑은 아니었더라 내 곁에 머물던 시간이었을 뿐 이제야 어렴풋이 알 것만 같아 왜 넌 미안했어야만 했는지 내가 너무 들떴었나 봐 떠나는 순간마저 기대를 했었다니 얼마나 우스웠던 거니 좋은 이별이란 거, 결국 세상엔 없는 일이라는 걸 알았다면 그때 차라리 다 울어둘 걸 그때 이미 나라는 건 네겐 끝이었다는 건 나만 몰랐었던 이야기\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막곡 가사 출력\n",
        "raw_corpus[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "4pC7Jhwhsjmj",
        "outputId": "be86155f-3f20-4090-cd84-54517c6c47fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'유난히 추운 날 따스한 햇살 같던 날 그대를 처음 마주했던 밤 웃음이 새어 나오고 설레는 맘을 간직했었던 그 날의 우리 점점 서로가 당연한 듯 익숙해질 때 그때 알았어야 했어 조금 섣불렀단 걸 다시는 사랑하지 않고 이별에 아파하기 싫어 내가 싫어 떠나간 그대를 기다리는 나 아닌 척하는 게 힘들어 아직도 네가 많이 보고 싶어 네 생각에 아파오는 나 그대를 탓하며 혼자서 원망했던 날 사실 이별을 인정하기 싫었어 차가워진 네 말투를 그럴 수 있다며 난 괜찮다고 아무렇지 않다고 사랑해 아직 잊지 못해 아직도 슬퍼하는 나야 돌이킬 수 없단 걸 알지만 사랑한다고 아닌 척하는 게 힘들어 아직도 네가 많이 보고 싶어 추억에 또 아파오는 나 떠나가던 날 붙잡을 수 없었던 나를 미워하며 보낸 지난날 그대가 내게 돌아오는 날만 기다려 찢어질 듯 아팠다고 날 사랑해달라고 미안해 아직 잊지 못해 아직도 슬퍼하는 나야 돌이킬 수 없단 걸 알지만 사랑한다고 아닌 척하는 게 힘들어 아직도 네가 많이 보고 싶어 추억에 또 아파오는 나 이별에 또 울고 있는 나'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확인\n",
        "\n",
        "corpus= []\n",
        "\n",
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue           # 길이가 0인 문장 제외\n",
        "    if ('[' and ']') in sentence: continue    # 대괄호가 들어가는 문장 제외\n",
        "    \n",
        "    corpus.append(sentence)\n",
        "\n",
        "\n",
        "print(f'원래 데이터 크기 : {len(raw_corpus)}')\n",
        "print(f'공백인 문장, 대괄호 포함된 문장 제외한 데이터 크기 : {len(corpus)}')\n",
        "\n",
        "corpus = list(set(corpus))          # 중복불가한 set의 특성을 이용하여 중복되는 문장 제외 후 다시 리스트로 변환\n",
        "print(f'중복되는 문장 제외한 데이터 크기 : {len(corpus)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzjnASev3Ban",
        "outputId": "6d99a9fc-4f26-44ed-afb0-243b29baee57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 데이터 크기 : 474\n",
            "공백인 문장, 대괄호 포함된 문장 제외한 데이터 크기 : 474\n",
            "중복되는 문장 제외한 데이터 크기 : 474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###전처리"
      ],
      "metadata": {
        "id": "Ves3hLK3E-04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 정의\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()                     # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
        "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
        "    sentence = sentence.strip()                             # 5\n",
        "    # sentence = '<start> ' + sentence + ' <end>'             # 6\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "Rw8n0uMA8cG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tokenized = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    # 데이터 전처리\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus_tokenized.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10곡 확인\n",
        "corpus_tokenized[:10]"
      ],
      "metadata": {
        "id": "o-7bZKUp8wBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6da7b2-7d41-403f-93e3-b96e73fa7c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['니가 너무 예뻐서 그래 니가 너무 예뻐서 그래 나는 너만 보면 원래 나는 너한테만 더 그래 빛이나 너의 얼굴 몸매 굳이 과시하지마 뭐해 더 빠져들어가 너한테 미쳤나 모든게 완벽해 딴 남자 앞에선 차갑게 내 앞에서만 솔직하게 해 해줘 가끔 난 걱정해 넌 너무 빛나서 남자들 쳐다봐 난 다 마음에 안들어 because the way you dress and your body 남자들을 미치게 해 그래서 나까지 no need high heels 그냥 청바지와 운동화 and a little bit 액세서리 t shirt 만 입어도 you re perfect baby 이런 네가 좋아 i got some more to say 다른 남자들이 그냥 너를 볼 때 난 괜시리 불안해져 니꺼 라고 말해 나한테 매일 더 원하고 또 원해 다른 남자들이 그냥 너를 볼 때 난 괜시리 불안해서 내꺼 라고 말해 너한테 매일 너 하나만 생각해 니가 너무 예뻐서 그래 니가 너무 예뻐서 그래 나는 너만 보면 원래 나는 너한테만 더 그래 미쳤나 보면 볼수록 더 머릿속 너로 가득 채워 심장까지 번져버렸어 내 숨소리만 더 가빠져 aye girl 네가 정말 눈 부시잖아 너의 아름다운 미모 빛이 나잖아 all them other guys 지나가면 반해 버려서 나는 불안 하게 돼 네게 빠진단 말이야 네 옆에 있고 싶어 널 보는 시선 너무 많아 여기 저기 거기 전부 다 남자들이잖아 so please 두 손 깍지 끼고 빌어 that you will stay by my side 제발 나를 꽉 안아 다른 남자들이 그냥 너를 볼 때 난 괜시리 불안해져 니꺼 라고 말해 나한테 매일 더 원하고 또 원해 다른 남자들이 그냥 너를 볼 때 난 괜시리 불안해서 내꺼 라고 말해 너한테 매일 너 하나만 생각해 너무 빛이나 내곁에 있어줘 너무 빛이나 나만을 밝혀줘 빛이나 네가 너무 빛이나 누가 봐도 넌 빛이나 널 볼수록 더 빛이나 baby 니가 너무 예뻐서 그래 니가 너무 예뻐서 그래 나는 너만 보면 원래 나는 너한테만 더 그래',\n",
              " '그 날이 그리워 울려 퍼지는 속삭임 그대가 그리워 두 발길 따라 걷는 소나기 난 난 가야지 그 날이 그리워 그대가 그리워 오늘 같은 날씨 바람 부는 저 들길 끝에는 삼포로 가는 길 있겠지 굽이굽이 산길 걷다 보면 한 발 두 발 한숨만 나오네 아아 뜬구름 하나 삼포로 가거든 정든 님 소식 좀 전해주렴 나도 따라 삼포로 간다고 사랑도 이젠 소용없네 삼포로 나는 가야지 때마침 내게 오라는 당기네 삼포의 내음 yeah 왜 또 참을 만하면 날 또 찾아오네 이 솔직한 그리움 삼포로 가는 길 참고로 깊숙이 자리해 손짓한 그곳엔 내 님이 있길 그곳에 내 님이 있길 낭만과의 향연 그 속에 춤추는 장면 찬란하게 빛나는 그 모습에 나도 따라 가겠소 그대 역시도 당연 저 산마루 쉬어간 길손아 내 사연 전해 듣겠소 정든 고향 떠난 지 오래고 내 님은 소식도 몰라요 아아 뜬구름 하나 삼포로 가거든 정든 님 소식 좀 전해주렴 나도 따라 삼포로 간다고 아아 뜬구름 하나 삼포로 가거든 정든 님 소식 좀 전해주렴 나도 따라 삼포로 간다고 사랑도 이젠 소용없네 삼포로 나는 가야지 삼포로 삼포로 나는 가야지 그 날이 그리워 그 날이 그리워 그대가 그리워 그 날이 그리워 그대가 그리워 삼포로 나는 가야지',\n",
              " '조만간 얼굴 보자 언제 밥 한번 먹자 좋아 좋아 난 오늘도 기다려 더 예뻐진 것 같네 뭐가 이리 달콤해 so sweet so sweet 그럼 나 또 기대해 빠라밤 빠라밤 빠라밥 빠라밥 빠라밤 이번에는 진심이 맞죠 거짓말 아니죠 진짜 보는 거죠 거짓말이라도 일단은 삼키고 볼래요 맘이 썩는 것도 아닌데 뭐 어때 빈말이라도 난 좋아 빈말이라도 난 좋아 그 말 한마디를 잡고 며칠은 꿈꿀 수 있어 몇 마디라도 난 좋아 몇 마디라도 난 좋아 내 하룰 열고 하룰 닫는 말 그 말 담에 한번 보자고 말한 게 언제 인지 몰라 몰라 난 오늘 또 기다려 점점 더 예뻐진대 그 말만 열두 번째 싫어 싫어 저 영혼 없는 말투 빠라밤 빠라밤 빠라밥 빠라밥 빠라밤 이번에도 진심 아니죠 거짓말인 거죠 이젠 안 속아요 달콤한 말들만 삼키고 삼켰던 내 맘이 까맣게 썩어가는 줄도 모르고 빈말이 난 이젠 싫어 빈말이 난 이제 싫어 그 말 한마디를 잡고 밤새 헛된 꿈꾸게 해 빈말 이젠 정말 싫어 빈말 이젠 정말 싫어 텅텅 빈 하트같이 공허한 그 말 빈말 한마디로 하룰 버티던 나인데 어느새 기대가 커지고 커져서 툭툭 뱉고 사라진 널 미워하게 된 건 내가 변한 걸까 내 욕심이 커져버린 걸까 빈말이 난 이젠 싫어 빈말이 난 이제 싫어 그 말 한마디를 잡고 밤새 헛된 꿈꾸게 해 빈말 이젠 정말 싫어 빈말 이젠 정말 싫어 텅텅 빈 하트같이 공허한 그 말',\n",
              " '밤이 깊어 어둠마저 잠들면 방 안 가득 시계소리만 왼쪽 가슴에 손을 올려봐 내가 아직도 숨을 쉬는지 못났다 참 못났다 이 정도였니 뭐 대단한 사랑이라고 울지마 다 쓰고 없다 이제 사랑은 없다 남김없이 너에게 다 더 이상 가진 게 없으니까 나의 세상에 빛이었던 한 사람 이젠 니가 없다 서러워 참 서러워 그게 다였니 뭐 특별한 사랑이라고 결국 다 끝인걸 다 쓰고 없다 이제 사랑은 없다 남김없이 너에게 다 더 이상 가진 게 없으니까 나의 세상에 빛이었던 한 사람 이젠 니가 없다 위태로워 앞이 안보일 벼랑인걸 알면서 니가 먼저 내 손 놓아버릴 거란 걸 알면서 아주 짧은 순간조차 그게 너라면 미련하게 또 미련하게 널 사랑할거야 남은 게 없다 이제 아무도 없다 너 아니면 너 아니면 나에게 사랑은 없으니까 빛은 사라져 더는 갈 수가 없어 난 널 기다린다',\n",
              " '뜬 눈으로 이 밤 지새웠지만 싫지는 않아 어제 마신 커피 때문은 아냐 오늘 너와의 첫 데이트 날인걸 어색할 것 같지만 괜찮아 너의 두 눈만 볼 수 있다면 정말이야 이런 내가 아닌데 벌써 네가 보고파서 먼저 도착해 oh my baby 나 왜이래 너밖에 안보여 every time everywhere 네 생각만 나 매일 해 너와 처음 만났었던 그 때부터 지금까지 네 머리끝부터 네 발 끝까지 다 내 꺼 내 꺼 내 꺼 yeah 네 화장기 없는 그 모습까지 다 예뻐 예뻐 예뻐 yeah 넌 새로 산 신발처럼 clean 피웅피웅 폭죽과 터지는 샴페인 no break you say j i say 애기 여보 자기 ay girl 살짝 가볍고 달콤한 대화 틱탁 거리던 시간도 쏟아지듯이 흘러가는걸 어느 걸 그룹의 소녀가 와도 쉽게 내 맘을 못 바꿀걸 준비한 건 없지만 널 위해 만든 이 노랠 불러주고 싶어 정말이야 이런 내가 아닌데 자꾸 귓가에 맴도는 너의 멜로디 oh my baby 나 왜이래 너밖에 안보여 every time everywhere 네 생각만 나 매일 해 너와 처음 만났었던 그 때부터 지금까지 네 머리끝부터 네 발 끝까지 다 내 꺼 내 꺼 내 꺼 yeah 네 화장기 없는 그 모습까지 다 예뻐 예뻐 예뻐 yeah 머리 어깨 무릎 발 무릎 발 하나부터 열까지 모든 게 look so fine 매일 아침을 너 생각에 미소로 맞이해 내 모든걸 쏟았는데 욕심이나니 왜 콩깍지 제대로 씌었나 봐 시간이 너무 빨라 너랑 있을 때마다 남자 너무 많아 걔랑 연락 끊어 나 말고 어디가 now stop 일루와 oh my baby 나 왜이래 너밖에 안보여 every time everywhere 네 생각만 나 매일 해 너와 처음 만났었던 그 때부터 지금까지 네 머리끝부터 네 발 끝까지 다 내 꺼 내 꺼 내 꺼 yeah 네 화장기 없는 그 모습까지 다 예뻐 예뻐 예뻐 yeah',\n",
              " '소중했던 기억들까지 over 행복했던 순간들까지 over 가슴 속에 깊이 박혀 있는 너 꺼내려 할수록 깊어져 상처만이 커져 보여서 울어 가지 말라 말해도 더 가지 말라 말해도 두 귀를 가렸는지 넌 뒤돌아 떠나 버렸어 가끔 너를 원망해 그 시간 빼고 사랑해 반지를 꽤 멀리로 내던지고 또 다시 껴 정말 난 괜찮다는 뻔한 거짓말은 그만해 제발 돌아와 줘 나를 꼭 안아 줘 미안해 소중했던 기억들까지 over 행복했던 순간들까지 over 가슴 속에 깊이 박혀 있는 너 꺼내려 할수록 깊어져 상처만이 커져 보여서 울어 새벽 3시 반 니 생각에 또 괴로워 나 매번 괜찮다고 속으로 되뇌었지만 항상 니 옆에 있을 때 never felt so alive 내 인생에 비춰줬던 star in my sky come back to me 왜 떠나간 건데 텅 빈 내 옆자리 맘이 허전해 this was our true love 믿음직한 사랑 난 너 밖에 모르는 바보잖아 정말 난 괜찮다는 뻔한 거짓말은 그만해 제발 돌아와 줘 나를 꼭 안아 줘 미안해 소중했던 기억들까지 over 행복했던 순간들까지 over 가슴 속에 깊이 박혀 있는 너 꺼내려 할수록 깊어져 상처만이 커져 보여서 울어 다 미안 지난 시간 나 니맘 아프게 해서 내가 미워 너무 미워 널 사랑하면 할 수록 더 떠나는 뒷 모습에 못이 박혀 가슴에 뒤돌아 뱉은 한숨에 심장이 찢겨 단숨에 아직도 너만을 바라보는 이 두 눈을 책임져 네 사랑 앞에서 문 두드리는 점점 더 소중했던 기억들까지 over 행복했던 순간들까지 over 가슴 속에 깊이 박혀 있는 너 꺼내려 할수록 깊어져 상처만이 커져 보여서 울어 true love true love true love true love true love true love true love true love true love',\n",
              " '니 얼굴 니 웃음 그 목소리 어쩔 수 없어 벌써 아침이잖아 이젠 익숙해 요즘 따라 왜 잠 못 드는 날 뿐이야 또 니가 생각나서 그래 잊은 거라 믿어 왔는데 괜찮은 척 하면서 baby 날 달래고 익숙하게 반복되는 거짓말 사실 이젠 너 때문에 내가 웃질 못해 이렇게 아프게 지내 온 종일 그만 잠들고 싶은데 눈 감을 수가 없는 건 더욱 선명해지는 니 얼굴 니 웃음 그 목소리 니 얼굴 때문에 니 얼굴 니 웃음 그 목소리 니 모습 때문에 너무나 선명해 난 아직까지도 잊을 수 없는 걸 잊을 수 없어 어떻게 나를 달래도 널 지울 수가 없어 아무 소용 없어 하나 둘 아련했던 너와 내 모습이 눈을 감으면 너무 선명해서 눈부시던 너와 설레던 내가 아직 그대로 내 안에 있어 니가 또 그리워져 이젠 지운 거라 믿어 왔는데 어쩔 수 없나 봐 난 baby 너의 기억 흔적들로 어떻게든 살아가 사실 이젠 너 때문에 내가 웃질 못해 이렇게 아프게 지내 온 종일 그만 잠들고 싶은데 눈 감을 수가 없는 건 더욱 니가 더 선명해져서 아직 곁에 있을 것만 같아 예전 모습 그대로 매일 밤 너를 또 찾아 그만 잊어야 하는데 난 지울 수가 없는 건 더욱 선명해지는 니 얼굴 니 웃음 그 목소리 니 얼굴 때문에 니 얼굴 니 웃음 그 목소리 니 모습 때문에 너무나 선명해 난 아직까지도 잊을 수 없는 걸 잊을 수 없어',\n",
              " '네가 떠나든 말든 난 상관없어 무거운 짐을 떼어낸 것처럼 맘이 후련해 i know i know 이 다짐도 부질없어 제자릴 맴돌 뿐 똑같은 마음 꼭 느끼길 원해 멀어질까 두려운 감정에 움츠려들 때 i do i do 널 봐주지 않을 거야 please let me go away 난 아냐 angel 난 아냐 그런 사람 네 앞에서 태연할 수 없는 상황 못된 감정이 앞서 불행했음 싶어 너 땜에 내가 변할까 봐 또 난 애써 미소 짓네 쉽게 지나갈 기분은 아닌가 봐 아침이 밝아도 눈앞이 캄캄해지네 i know i know 이러는 건 촌스럽죠 근데 뭐 어쩌겠어 난 아냐 angel 난 아냐 그런 사람 네 앞에서 태연할 수 없는 상황 못된 감정이 앞서 불행했음 싶어 너 땜에 내가 변할까 봐 또 난 애써 미소 짓네 whatever 네가 느껴져 whenever 자꾸 떠올라서 화를 내고 울지도 못해 흔들림을 감출 수 없네 i wanna love 착한 척만 할 뿐 난 아냐 angel 네 곁에선 행복 할 수 없어 못된 감정이 앞서 불행했음 싶어 너 땜에 내가 변할까 봐 또 난 애써 미소 짓네 angel',\n",
              " '그리운다 함께 했던 시간들 그리운다 함께 했던 그날들 그땐 네게 말 못한 차마 하지 못한 말 행복했던 그때처럼 우리 사랑할 수 있을까 그리운다 또 아프고 아파서 내 지워지지 않는 상처가 그리워서 운다 너를 부른다 하염없이 널 또 외쳐본다 가슴 터질 것처럼 너에게 닿을 수 없을 때 내 맘이 아파하는데 내 기억 속에서 많이 사랑했을 때에 넌 그때에 우리에게 그땐 너에게 정말 사랑했냐고 이제 오늘도 그리워운다 그리운다 그리운다 또 아프고 아파서 내 지워지지 않는 상처가 그리워서 운다 너를 부른다 하염없이 널 또 외쳐본다 가슴 터질 것처럼 너에게 닿을 수 없을 때 내 맘이 아파하는데 내 기억 속에서 많이 사랑했을 때에 넌 그때에 우리에게 그땐 너에게 정말 사랑했냐고 이제 오늘도 그리워 운다 그리운다 하루만 보고 싶은데 이렇게 널 지우면 내 맘이 아파오는데 내가 널 지우면 내가 널 잊는다면 하루도 살아갈 자신이 없는데 너에게 닿을 수 없을 때 내 맘이 아파하는데 내 기억 속에서 많이 사랑했을 때에 넌 그때에 우리에게 그땐 너에게 정말 사랑했냐고 이제 오늘도 그리워 운다 그리운다',\n",
              " '영화 속에 있는 것 같아 이렇게 그대와 함께 있을 때면 평범한 하루 기적이 되죠 너라는 사람 옆에 있으면 겨울 같았던 내 하루하루가 그대란 봄을 만나서 화사해져요 달콤해요 내 맘이 웃어요 세상 모든 게 다 너로 보인다 나만 사랑하던 나였죠 나라는 세상에 갇혀 있었죠 가슴이 뛰어 자꾸만 설레 그대라는 세상이 궁굼해 겨울 같았던 내 하루하루가 그대란 봄을 만나서 화사해져요 달콤해요 내 맘이 웃어요 세상 모든 게 다 너로 보인다 어렵기만 하던 세상이 착해 보여요 무엇도 두렵지 않아 멋진 사람이 될래 더 좋은 사람이 될래 그댄 내 삶에 기적인 거죠 지루했었던 내 하루하루가 그대란 기적을 만나 반짝거려요 평범했던 하루의 끝에서 나는 가슴이 또 두근거려요 사랑합니다 널 사랑합니다 그대가 옆에 있으면 용감해져요 살다보면 슬픔도 오겠죠 너와 함께라면 이젠 웃어요']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토큰화1. 한국어 형태소 분석 (okt)"
      ],
      "metadata": {
        "id": "fbExrAqKLKDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소분석기 설치\n",
        "# 형태소분석기 설치\n",
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
      ],
      "metadata": {
        "id": "yJj9MTwxs8V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "\n",
        "# kkm = Kkma()\n",
        "# kom = Komoran()\n",
        "# mec = Mecab()\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "w8W-HXFAsh06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# okt 토큰화\n",
        "ko_tokenized = []\n",
        "\n",
        "for c in corpus_tokenized:\n",
        "   ko_tokenized.append(okt.morphs(c))"
      ],
      "metadata": {
        "id": "y21cK8izHU4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 곡의 시작과 끝을 <start>와 <end>로 표시\n",
        "\n",
        "song_list = []\n",
        "for song in ko_tokenized:\n",
        "  song.insert(0, '<start>')\n",
        "  song.append('<end>')\n",
        "  song_list.append(song)"
      ],
      "metadata": {
        "id": "dWgzb-6yTJzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화된 각 곡의 앞 뒤에 <start>, <end>가 잘 추가 되었는지 개수로 확인해보기\n",
        "\n",
        "print('총 곡의 수: ',len(song_list))\n",
        "print('<start>의 개수: ',sum(list(map(lambda x: x.count('<start>'), song_list))))\n",
        "print('<end>의 개수: ',sum(list(map(lambda x: x.count('<end>'), song_list))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y7TWc-rp8ar",
        "outputId": "baa2b44d-3383-42ea-9e88-59607adab3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 곡의 수:  474\n",
            "<start>의 개수:  474\n",
            "<end>의 개수:  474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###토큰화2. 단어장 만들기 (tokenizer)"
      ],
      "metadata": {
        "id": "lCJxWCtx80zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=15000,         # 15000개 단어를 기억할 수 있음\n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"        # 포함되지 않는 단어는 <unk> 으로 표현\n",
        "    )\n",
        "    \n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  # 토큰 15개 초과 제외\n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer"
      ],
      "metadata": {
        "id": "SWNt9RyL8xn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor, tokenizer = tokenize(song_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MgflVmmNIp6",
        "outputId": "7b354816-dab6-4c0a-b549-1fde47917625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   8   18    3 ...   28  177   33]\n",
            " [ 372   11   17 ... 1243  123   33]\n",
            " [ 111  265 1248 ...   11   12   33]\n",
            " ...\n",
            " [ 348  357  330 ...  190   74   33]\n",
            " [ 695  167  533 ...   38 2265   33]\n",
            " [1102   27  314 ...  181   67   33]] <keras_preprocessing.text.Tokenizer object at 0x7f4863d318d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어장의 1~10번째 단어 확인해보기\n",
        "\n",
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mTMgxma8-BK",
        "outputId": "f0b8c951-1fcb-4db3-e187-b868c5bce933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : 이\n",
            "3 : 너\n",
            "4 : 내\n",
            "5 : 에\n",
            "6 : 가\n",
            "7 : 을\n",
            "8 : 나\n",
            "9 : 사랑\n",
            "10 : 의\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = tensor[:, :-1] # 소스문장 생성 (마지막 단어 1개 빼고 다 가져오기)\n",
        "\n",
        "tgt_input = tensor[:, 1:]  # 타켓문장 생성 (첫번째 단어 1개 빼고 다 가져오기)\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSZTTx7j9Boy",
        "outputId": "de6e8f65-4d2f-4164-a631-c2b77ab81825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  8  18   3  24 254 639   8  18   3  29 411  24  28 177]\n",
            "[ 18   3  24 254 639   8  18   3  29 411  24  28 177  33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소스문장 인덱스 -> 단어로 변환\n",
        "for idx in src_input[0]:\n",
        "  print(tokenizer.index_word[idx], end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDbWbd-5Nxe5",
        "outputId": "0777fed5-ffb4-422c-d5cb-c5a7df622325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나 는 너 만 보면 원래 나 는 너 한 테 만 더 그래 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 타켓문장 인덱스 -> 단어로 변환\n",
        "for idx in tgt_input[0]:\n",
        "  print(tokenizer.index_word[idx], end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQdWyA1LPOJr",
        "outputId": "b7c57d3b-810e-4df5-9381-63f3aa211af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "는 너 만 보면 원래 나 는 너 한 테 만 더 그래 <end> "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "pQoLn0jkCrkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,       # 데이트셋 비율\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=121)     # 결과를 일정하게 보여주기위해 지정"
      ],
      "metadata": {
        "id": "M-EyEKd_9HbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I1dHUNb9MC4",
        "outputId": "c5a1ede2-8645-4f5c-d26f-759bbb5ca916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Train: (379, 14)\n",
            "Target Train: (379, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###모델학습"
      ],
      "metadata": {
        "id": "wBVWRPyPCxe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding() - 단어를 밀집 벡터로 만드는 역할\n",
        "#             - 인공 신경망 용어로는 임베딩 층(embedding layer)을 만드는 역할을 합니다. \n",
        "#             - Embedding()은 정수 인코딩이 된 단어들을 입력을 받아서 임베딩을 수행합니다.\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        \n",
        "        self.embedding = Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "aSZ3SHX89OD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.num_words + 1\n",
        "embedding_size = 700\n",
        "hidden_size = 350\n",
        "# drop_rate = 0.2\n",
        "model = TextGenerator(vocab_size, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "Uvj95L3NC4BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.nn_ops import dropout\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train, epochs=200, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lwC5wq5DIvw",
        "outputId": "9c71c967-bb48-4a05-9577-4366b35b64bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 8s 24ms/step - loss: 9.2226\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 7.1783\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.6736\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 6.5038\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.4213\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3929\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 6.3745\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3607\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3604\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3499\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3475\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3500\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3423\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3410\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3396\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3379\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3382\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3210\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3130\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.2987\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.2878\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.2748\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.2585\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.2389\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.2093\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.1794\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.1661\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 6.1442\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.1233\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.0887\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.0658\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.0352\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 6.0124\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.9953\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.9623\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.9341\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.9101\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.8779\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.8440\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.8029\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.7620\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.7281\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.7072\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.6617\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.6163\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.5672\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.5284\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.4941\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.4422\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.4012\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.3638\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.3340\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.2930\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.2501\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.2037\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.1527\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.1098\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.0726\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.0253\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 4.9878\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.9619\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.9234\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.8816\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.8456\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.8034\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.7675\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.7311\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 4.7002\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.6559\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.6272\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.5897\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.5517\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 4.5223\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.4893\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.4605\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 4.4363\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 4.4063\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.3689\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.3345\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 4.2838\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.2434\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.2047\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.1661\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.1322\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.1003\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 4.0680\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 4.0346\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.9965\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.9630\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.9235\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.8963\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.8629\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.8212\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.7984\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.7900\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.7605\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.7355\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.7038\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.6612\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.6254\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.5775\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.5314\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 3.4913\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.4469\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.4084\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.3713\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.3468\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.3058\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.2694\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.2307\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.1950\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.1648\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.1243\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.0888\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.0639\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.0312\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.9957\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.9578\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.9265\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.8920\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.8692\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.8516\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.8164\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.7768\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.7449\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.7091\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.6771\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.6428\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.6085\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.5677\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.5351\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.5039\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.4587\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.4288\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.3987\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.3644\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.3285\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.2962\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.2514\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.2183\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.1888\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.1545\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.1264\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.0949\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.0635\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.0309\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.0061\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.9828\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.9511\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.9207\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.8981\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.8689\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.8461\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.8181\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.7902\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.7627\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.7298\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.7025\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.6771\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.6478\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.6248\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.6096\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.5909\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.5604\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.5284\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.4985\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.4740\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.4557\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.4368\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.4055\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.3809\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.3576\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.3281\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.2966\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.2727\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.2581\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.2435\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.2206\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.1972\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.1762\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.1546\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.1305\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.1116\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0857\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0660\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0488\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0282\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0088\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9884\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9688\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9514\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9358\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9200\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9031\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.8868\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.8756\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.8627\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.8558\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.8479\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.8257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee28806790>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 생성 함수\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
        "    while True:\n",
        "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
        "\n",
        "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "\n",
        "    generated = \"\"\n",
        "        # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        if word_index == 0:\n",
        "            continue\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated #최종적으로 모델이 생성한 자연어 문장\n"
      ],
      "metadata": {
        "id": "WPsahH_FDKt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과 (가사 생성)"
      ],
      "metadata": {
        "id": "AEjcptLsaPpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 발라드 474곡으로 200번 학습한 결과, okt 사용, <start>, <end> 추가 \n",
        "\n",
        "print('예시1.', generate_text(model, tokenizer, init_sentence='힘내', max_len=50))\n",
        "print('예시2.', generate_text(model, tokenizer, init_sentence='슬픔', max_len=50))\n",
        "print('예시3.', generate_text(model, tokenizer, init_sentence='i love', max_len=50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wM5PFIjGgFB",
        "outputId": "49eb48ec-88bb-4425-9173-e016c91554e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시1. 힘내 사랑 에 젠 하는 더 건 더 더 i 까지 you <end> \n",
            "예시2. 슬픔 상처 에 지우죠 그 널 사랑 한다 더 까지 <end> \n",
            "예시3. i love and i love time believe 뭘 over you feel first kiss <end> \n",
            "예시3. <unk> 내려 a 진작 를 그렇게 보고싶다 말 해줬으면 이렇게 끝나진 않을 걸 이 젠 늦었어 <end> \n"
          ]
        }
      ]
    }
  ]
}